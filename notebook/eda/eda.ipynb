{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3571d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mh_sys_gen import MHSysGen\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e214f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 150 rows of test data in: test_input1.csv\n",
      "File has 132 columns (should be 132 features + TransactionID).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Define the 133 Required Columns (Copy-pasted from your main.py) ---\n",
    "ALL_REQUIRED_COLUMNS = [\n",
    "    'TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', \n",
    "    'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'dist1', 'dist2',\n",
    "    'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6',\n",
    "    'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4',\n",
    "    'D5', 'D8', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3', 'M4',\n",
    "    'M5', 'M6', 'V6', 'V12', 'V13', 'V19', 'V20', 'V35', 'V38', 'V45', 'V48',\n",
    "    'V53', 'V54', 'V55', 'V56', 'V61', 'V62', 'V66', 'V67', 'V70', 'V73',\n",
    "    'V75', 'V76', 'V78', 'V82', 'V83', 'V87', 'V91', 'V94', 'V96', 'V99',\n",
    "    'V115', 'V126', 'V128', 'V131', 'V133', 'V136', 'V139', 'V143', 'V149',\n",
    "    'V156', 'V165', 'V169', 'V171', 'V187', 'V189', 'V197', 'V206', 'V220',\n",
    "    'V256', 'V258', 'V261', 'V262', 'V281', 'V283', 'V285', 'V291', 'V294',\n",
    "    'V295', 'V296', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313',\n",
    "    'V314', 'V315', 'V317', 'V320', 'V326', 'id_01', 'id_02', 'id_03', 'id_04',\n",
    "    'id_05', 'id_06', 'id_09', 'id_13', 'id_14', 'id_17', 'id_19', 'id_20',\n",
    "    'id_30', 'id_31', 'id_32', 'id_33', 'DeviceInfo'\n",
    "]\n",
    "\n",
    "# The model input features (excluding 'isFraud')\n",
    "MODEL_FEATURES = [col for col in ALL_REQUIRED_COLUMNS if col not in ['isFraud']]\n",
    "\n",
    "NUM_ROWS = 150\n",
    "\n",
    "# --- 2. Generate Dummy Data ---\n",
    "data = {}\n",
    "\n",
    "# TransactionID: Unique integers starting from a high number\n",
    "data['TransactionID'] = np.arange(1000000, 1000000 + NUM_ROWS)\n",
    "\n",
    "# TransactionAmt: Random floats\n",
    "data['TransactionAmt'] = np.round(np.random.uniform(10.0, 500.0, NUM_ROWS), 2)\n",
    "\n",
    "# ProductCD, card4, card6 (Categorical/Object features)\n",
    "data['ProductCD'] = np.random.choice(['W', 'C', 'R', 'H', 'S'], NUM_ROWS)\n",
    "data['card4'] = np.random.choice(['visa', 'mastercard', 'amex', 'discover'], NUM_ROWS)\n",
    "data['card6'] = np.random.choice(['debit', 'credit'], NUM_ROWS)\n",
    "\n",
    "# C1-C14 (Count features - typical fraud variables)\n",
    "c_cols = [f'C{i}' for i in range(1, 7)] + [f'C{i}' for i in range(8, 15)]\n",
    "for col in c_cols:\n",
    "    data[col] = np.random.randint(1, 100, NUM_ROWS)\n",
    "\n",
    "# V-features and other numerical columns (Fill with random floats/NaNs to test imputation)\n",
    "numerical_cols = [col for col in MODEL_FEATURES if col not in data]\n",
    "for col in numerical_cols:\n",
    "    # Introduce some NaNs (10% chance) to test the imputation logic in your preprocessor\n",
    "    values = np.random.uniform(0.0, 500.0, NUM_ROWS)\n",
    "    values[np.random.choice(NUM_ROWS, size=int(NUM_ROWS * 0.10), replace=False)] = np.nan\n",
    "    data[col] = values\n",
    "\n",
    "# --- 3. Create DataFrame and Output CSV ---\n",
    "\n",
    "# Use only the features the model expects, and ensure the order is correct\n",
    "df_test = pd.DataFrame(data, columns=MODEL_FEATURES)\n",
    "\n",
    "# OPTIONAL: Drop a critical column to intentionally test the 'missing column' logic in FastAPI\n",
    "# df_test = df_test.drop(columns=['D15']) \n",
    "\n",
    "output_filename = 'test_input1.csv'\n",
    "df_test.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Successfully generated {NUM_ROWS} rows of test data in: {output_filename}\")\n",
    "print(f\"File has {len(df_test.columns)} columns (should be 132 features + TransactionID).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb3561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd92eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"TransactionID\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b593de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 590540 entries, 0 to 590539\n",
      "Data columns (total 132 columns):\n",
      " #    Column          Non-Null Count   Dtype  \n",
      "---   ------          --------------   -----  \n",
      " 0    isFraud         590540 non-null  int64  \n",
      " 1    TransactionDT   590540 non-null  int64  \n",
      " 2    TransactionAmt  590540 non-null  float64\n",
      " 3    ProductCD       590540 non-null  object \n",
      " 4    card1           590540 non-null  int64  \n",
      " 5    card2           590540 non-null  float64\n",
      " 6    card3           590540 non-null  float64\n",
      " 7    card4           590540 non-null  object \n",
      " 8    card5           590540 non-null  float64\n",
      " 9    card6           590540 non-null  object \n",
      " 10   addr1           590540 non-null  float64\n",
      " 11   dist1           590540 non-null  float64\n",
      " 12   dist2           590540 non-null  float64\n",
      " 13   P_emaildomain   590540 non-null  object \n",
      " 14   R_emaildomain   590540 non-null  object \n",
      " 15   C1              590540 non-null  float64\n",
      " 16   C2              590540 non-null  float64\n",
      " 17   C3              590540 non-null  float64\n",
      " 18   C4              590540 non-null  float64\n",
      " 19   C5              590540 non-null  float64\n",
      " 20   C6              590540 non-null  float64\n",
      " 21   C8              590540 non-null  float64\n",
      " 22   C9              590540 non-null  float64\n",
      " 23   C10             590540 non-null  float64\n",
      " 24   C11             590540 non-null  float64\n",
      " 25   C12             590540 non-null  float64\n",
      " 26   C13             590540 non-null  float64\n",
      " 27   C14             590540 non-null  float64\n",
      " 28   D1              590540 non-null  float64\n",
      " 29   D2              590540 non-null  float64\n",
      " 30   D3              590540 non-null  float64\n",
      " 31   D4              590540 non-null  float64\n",
      " 32   D5              590540 non-null  float64\n",
      " 33   D8              590540 non-null  float64\n",
      " 34   D10             590540 non-null  float64\n",
      " 35   D11             590540 non-null  float64\n",
      " 36   D12             590540 non-null  float64\n",
      " 37   D13             590540 non-null  float64\n",
      " 38   D14             590540 non-null  float64\n",
      " 39   D15             590540 non-null  float64\n",
      " 40   M2              590540 non-null  object \n",
      " 41   M3              590540 non-null  object \n",
      " 42   M4              590540 non-null  object \n",
      " 43   M5              590540 non-null  object \n",
      " 44   M6              590540 non-null  object \n",
      " 45   V6              590540 non-null  float64\n",
      " 46   V12             590540 non-null  float64\n",
      " 47   V13             590540 non-null  float64\n",
      " 48   V19             590540 non-null  float64\n",
      " 49   V20             590540 non-null  float64\n",
      " 50   V35             590540 non-null  float64\n",
      " 51   V38             590540 non-null  float64\n",
      " 52   V45             590540 non-null  float64\n",
      " 53   V48             590540 non-null  float64\n",
      " 54   V53             590540 non-null  float64\n",
      " 55   V54             590540 non-null  float64\n",
      " 56   V55             590540 non-null  float64\n",
      " 57   V56             590540 non-null  float64\n",
      " 58   V61             590540 non-null  float64\n",
      " 59   V62             590540 non-null  float64\n",
      " 60   V66             590540 non-null  float64\n",
      " 61   V67             590540 non-null  float64\n",
      " 62   V70             590540 non-null  float64\n",
      " 63   V73             590540 non-null  float64\n",
      " 64   V75             590540 non-null  float64\n",
      " 65   V76             590540 non-null  float64\n",
      " 66   V78             590540 non-null  float64\n",
      " 67   V82             590540 non-null  float64\n",
      " 68   V83             590540 non-null  float64\n",
      " 69   V87             590540 non-null  float64\n",
      " 70   V91             590540 non-null  float64\n",
      " 71   V94             590540 non-null  float64\n",
      " 72   V96             590540 non-null  float64\n",
      " 73   V99             590540 non-null  float64\n",
      " 74   V115            590540 non-null  float64\n",
      " 75   V126            590540 non-null  float64\n",
      " 76   V128            590540 non-null  float64\n",
      " 77   V131            590540 non-null  float64\n",
      " 78   V133            590540 non-null  float64\n",
      " 79   V136            590540 non-null  float64\n",
      " 80   V139            590540 non-null  float64\n",
      " 81   V143            590540 non-null  float64\n",
      " 82   V149            590540 non-null  float64\n",
      " 83   V156            590540 non-null  float64\n",
      " 84   V165            590540 non-null  float64\n",
      " 85   V169            590540 non-null  float64\n",
      " 86   V171            590540 non-null  float64\n",
      " 87   V187            590540 non-null  float64\n",
      " 88   V189            590540 non-null  float64\n",
      " 89   V197            590540 non-null  float64\n",
      " 90   V206            590540 non-null  float64\n",
      " 91   V220            590540 non-null  float64\n",
      " 92   V256            590540 non-null  float64\n",
      " 93   V258            590540 non-null  float64\n",
      " 94   V261            590540 non-null  float64\n",
      " 95   V262            590540 non-null  float64\n",
      " 96   V281            590540 non-null  float64\n",
      " 97   V283            590540 non-null  float64\n",
      " 98   V285            590540 non-null  float64\n",
      " 99   V291            590540 non-null  float64\n",
      " 100  V294            590540 non-null  float64\n",
      " 101  V295            590540 non-null  float64\n",
      " 102  V296            590540 non-null  float64\n",
      " 103  V307            590540 non-null  float64\n",
      " 104  V308            590540 non-null  float64\n",
      " 105  V309            590540 non-null  float64\n",
      " 106  V310            590540 non-null  float64\n",
      " 107  V311            590540 non-null  float64\n",
      " 108  V312            590540 non-null  float64\n",
      " 109  V313            590540 non-null  float64\n",
      " 110  V314            590540 non-null  float64\n",
      " 111  V315            590540 non-null  float64\n",
      " 112  V317            590540 non-null  float64\n",
      " 113  V320            590540 non-null  float64\n",
      " 114  V326            590540 non-null  float64\n",
      " 115  id_01           590540 non-null  float64\n",
      " 116  id_02           590540 non-null  float64\n",
      " 117  id_03           590540 non-null  float64\n",
      " 118  id_04           590540 non-null  float64\n",
      " 119  id_05           590540 non-null  float64\n",
      " 120  id_06           590540 non-null  float64\n",
      " 121  id_09           590540 non-null  float64\n",
      " 122  id_13           590540 non-null  float64\n",
      " 123  id_14           590540 non-null  float64\n",
      " 124  id_17           590540 non-null  float64\n",
      " 125  id_19           590540 non-null  float64\n",
      " 126  id_20           590540 non-null  float64\n",
      " 127  id_30           590540 non-null  object \n",
      " 128  id_31           590540 non-null  object \n",
      " 129  id_32           590540 non-null  float64\n",
      " 130  id_33           590540 non-null  object \n",
      " 131  DeviceInfo      590540 non-null  object \n",
      "dtypes: float64(115), int64(3), object(14)\n",
      "memory usage: 594.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True,show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf4d25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         315.0\n",
       "1         325.0\n",
       "2         330.0\n",
       "3         476.0\n",
       "4         420.0\n",
       "          ...  \n",
       "590535    272.0\n",
       "590536    204.0\n",
       "590537    231.0\n",
       "590538    387.0\n",
       "590539    299.0\n",
       "Name: addr1, Length: 590540, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['addr1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8270d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature=[]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype==\"object\":\n",
    "        categorical_feature.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0aa5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         credit\n",
       "1         credit\n",
       "2          debit\n",
       "3          debit\n",
       "4         credit\n",
       "           ...  \n",
       "590535     debit\n",
       "590536     debit\n",
       "590537     debit\n",
       "590538     debit\n",
       "590539    credit\n",
       "Name: card6, Length: 590540, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['card6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "54e6d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud\n",
       "0    569877\n",
       "1     20663\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264453a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_cols=['ProductCD','card4','card6']#low cardinality categorical features\n",
    "nominal_cols_high=['P_emaildomain','R_emaildomain',\"id_30\",\"id_31\",\"DeviceInfo\",\"id_33\"]#high cardinality categorical features\n",
    "ordinal_cols=['M4','M2','M3','M5','M6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff925db",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols=[]\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols_high and col not in nominal_cols and col not in ordinal_cols and df[col].dtype!=\"object\":\n",
    "        if col!=\"isFraud\":\n",
    "         numeric_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74c94d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TransactionDT',\n",
       " 'TransactionAmt',\n",
       " 'card1',\n",
       " 'card2',\n",
       " 'card3',\n",
       " 'card5',\n",
       " 'addr1',\n",
       " 'dist1',\n",
       " 'dist2',\n",
       " 'C1',\n",
       " 'C2',\n",
       " 'C3',\n",
       " 'C4',\n",
       " 'C5',\n",
       " 'C6',\n",
       " 'C8',\n",
       " 'C9',\n",
       " 'C10',\n",
       " 'C11',\n",
       " 'C12',\n",
       " 'C13',\n",
       " 'C14',\n",
       " 'D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D8',\n",
       " 'D10',\n",
       " 'D11',\n",
       " 'D12',\n",
       " 'D13',\n",
       " 'D14',\n",
       " 'D15',\n",
       " 'V6',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V19',\n",
       " 'V20',\n",
       " 'V35',\n",
       " 'V38',\n",
       " 'V45',\n",
       " 'V48',\n",
       " 'V53',\n",
       " 'V54',\n",
       " 'V55',\n",
       " 'V56',\n",
       " 'V61',\n",
       " 'V62',\n",
       " 'V66',\n",
       " 'V67',\n",
       " 'V70',\n",
       " 'V73',\n",
       " 'V75',\n",
       " 'V76',\n",
       " 'V78',\n",
       " 'V82',\n",
       " 'V83',\n",
       " 'V87',\n",
       " 'V91',\n",
       " 'V94',\n",
       " 'V96',\n",
       " 'V99',\n",
       " 'V115',\n",
       " 'V126',\n",
       " 'V128',\n",
       " 'V131',\n",
       " 'V133',\n",
       " 'V136',\n",
       " 'V139',\n",
       " 'V143',\n",
       " 'V149',\n",
       " 'V156',\n",
       " 'V165',\n",
       " 'V169',\n",
       " 'V171',\n",
       " 'V187',\n",
       " 'V189',\n",
       " 'V197',\n",
       " 'V206',\n",
       " 'V220',\n",
       " 'V256',\n",
       " 'V258',\n",
       " 'V261',\n",
       " 'V262',\n",
       " 'V281',\n",
       " 'V283',\n",
       " 'V285',\n",
       " 'V291',\n",
       " 'V294',\n",
       " 'V295',\n",
       " 'V296',\n",
       " 'V307',\n",
       " 'V308',\n",
       " 'V309',\n",
       " 'V310',\n",
       " 'V311',\n",
       " 'V312',\n",
       " 'V313',\n",
       " 'V314',\n",
       " 'V315',\n",
       " 'V317',\n",
       " 'V320',\n",
       " 'V326',\n",
       " 'id_01',\n",
       " 'id_02',\n",
       " 'id_03',\n",
       " 'id_04',\n",
       " 'id_05',\n",
       " 'id_06',\n",
       " 'id_09',\n",
       " 'id_13',\n",
       " 'id_14',\n",
       " 'id_17',\n",
       " 'id_19',\n",
       " 'id_20',\n",
       " 'id_32']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c24b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=\"TransactionDT\")\n",
    "split_index=int(0.8*len(df))\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea7047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency encoding \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.freq_maps={}\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        X=pd.DataFrame(X)\n",
    "        for col in X.columns:\n",
    "            freq_map=X[col].value_counts(normalize=True)\n",
    "            self.freq_maps[col]=freq_map\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X=pd.DataFrame(X)\n",
    "        X_transformed=X.copy()\n",
    "        for col in X.columns:\n",
    "            X_transformed[col]=X_transformed[col].map(self.freq_maps[col]).fillna(0)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784624ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>DeviceInfo</th>\n",
       "      <th>id_33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outlook.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yahoo.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Android 7.0</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "      <td>2220x1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590535</th>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590536</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590537</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590538</th>\n",
       "      <td>aol.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590539</th>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590540 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       P_emaildomain R_emaildomain        id_30                id_31  \\\n",
       "0            unknown       unknown      unknown              unknown   \n",
       "1          gmail.com       unknown      unknown              unknown   \n",
       "2        outlook.com       unknown      unknown              unknown   \n",
       "3          yahoo.com       unknown      unknown              unknown   \n",
       "4          gmail.com       unknown  Android 7.0  samsung browser 6.2   \n",
       "...              ...           ...          ...                  ...   \n",
       "590535       unknown       unknown      unknown              unknown   \n",
       "590536     gmail.com       unknown      unknown              unknown   \n",
       "590537     gmail.com       unknown      unknown              unknown   \n",
       "590538       aol.com       unknown      unknown              unknown   \n",
       "590539     gmail.com       unknown      unknown              unknown   \n",
       "\n",
       "                           DeviceInfo      id_33  \n",
       "0                             unknown    unknown  \n",
       "1                             unknown    unknown  \n",
       "2                             unknown    unknown  \n",
       "3                             unknown    unknown  \n",
       "4       SAMSUNG SM-G892A Build/NRD90M  2220x1080  \n",
       "...                               ...        ...  \n",
       "590535                        unknown    unknown  \n",
       "590536                        unknown    unknown  \n",
       "590537                        unknown    unknown  \n",
       "590538                        unknown    unknown  \n",
       "590539                        unknown    unknown  \n",
       "\n",
       "[590540 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder ,OrdinalEncoder,StandardScaler\n",
    "ohe=OneHotEncoder(drop='first',sparse_output=False,handle_unknown='ignore')\n",
    "oe=OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1)\n",
    "scaler=StandardScaler()\n",
    "df[nominal_cols_high].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D8</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>M2</th>\n",
       "      <th>...</th>\n",
       "      <th>V256</th>\n",
       "      <th>V258</th>\n",
       "      <th>V261</th>\n",
       "      <th>V262</th>\n",
       "      <th>V281</th>\n",
       "      <th>V283</th>\n",
       "      <th>V285</th>\n",
       "      <th>V291</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V309</th>\n",
       "      <th>V310</th>\n",
       "      <th>V311</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V317</th>\n",
       "      <th>V320</th>\n",
       "      <th>V326</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_03</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_09</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40809</th>\n",
       "      <td>1008491</td>\n",
       "      <td>100.00</td>\n",
       "      <td>R</td>\n",
       "      <td>6177</td>\n",
       "      <td>399.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>american express</td>\n",
       "      <td>150.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>264.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>anonymous.com</td>\n",
       "      <td>anonymous.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>609.666687</td>\n",
       "      <td>15.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>58410.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-360.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>Windows 7</td>\n",
       "      <td>ie 11.0 for desktop</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1920x1080</td>\n",
       "      <td>Trident/7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285886</th>\n",
       "      <td>7008212</td>\n",
       "      <td>29.99</td>\n",
       "      <td>W</td>\n",
       "      <td>7900</td>\n",
       "      <td>345.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>143.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104256</th>\n",
       "      <td>2071522</td>\n",
       "      <td>107.95</td>\n",
       "      <td>W</td>\n",
       "      <td>11690</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>191.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>comcast.net</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.875000</td>\n",
       "      <td>502.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507860</th>\n",
       "      <td>13299752</td>\n",
       "      <td>241.95</td>\n",
       "      <td>W</td>\n",
       "      <td>2616</td>\n",
       "      <td>327.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>330.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.875000</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196382</th>\n",
       "      <td>4412283</td>\n",
       "      <td>117.00</td>\n",
       "      <td>W</td>\n",
       "      <td>13780</td>\n",
       "      <td>298.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>441.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionDT  TransactionAmt ProductCD  ...  id_32      id_33   DeviceInfo\n",
       "40809         1008491          100.00         R  ...   24.0  1920x1080  Trident/7.0\n",
       "285886        7008212           29.99         W  ...   24.0    unknown      unknown\n",
       "104256        2071522          107.95         W  ...   24.0    unknown      unknown\n",
       "507860       13299752          241.95         W  ...   24.0    unknown      unknown\n",
       "196382        4412283          117.00         W  ...   24.0    unknown      unknown\n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e4ea2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D8</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>M2</th>\n",
       "      <th>...</th>\n",
       "      <th>V256</th>\n",
       "      <th>V258</th>\n",
       "      <th>V261</th>\n",
       "      <th>V262</th>\n",
       "      <th>V281</th>\n",
       "      <th>V283</th>\n",
       "      <th>V285</th>\n",
       "      <th>V291</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V309</th>\n",
       "      <th>V310</th>\n",
       "      <th>V311</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V317</th>\n",
       "      <th>V320</th>\n",
       "      <th>V326</th>\n",
       "      <th>id_01</th>\n",
       "      <th>id_02</th>\n",
       "      <th>id_03</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_09</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_30</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316078</th>\n",
       "      <td>7864752</td>\n",
       "      <td>107.950</td>\n",
       "      <td>W</td>\n",
       "      <td>7585</td>\n",
       "      <td>553.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>264.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>aol.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.875</td>\n",
       "      <td>33.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116888</th>\n",
       "      <td>2260318</td>\n",
       "      <td>37.021</td>\n",
       "      <td>C</td>\n",
       "      <td>3154</td>\n",
       "      <td>408.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>hotmail.com</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>120692.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>chrome 63.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Windows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410211</th>\n",
       "      <td>10361225</td>\n",
       "      <td>59.000</td>\n",
       "      <td>W</td>\n",
       "      <td>10057</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>224.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>37.875</td>\n",
       "      <td>313.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251027</th>\n",
       "      <td>5974846</td>\n",
       "      <td>57.950</td>\n",
       "      <td>W</td>\n",
       "      <td>6207</td>\n",
       "      <td>355.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>143.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>538.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191265</th>\n",
       "      <td>4296816</td>\n",
       "      <td>107.950</td>\n",
       "      <td>W</td>\n",
       "      <td>11207</td>\n",
       "      <td>361.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>aol.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>37.875</td>\n",
       "      <td>288.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>T</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>125800.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>24.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionDT  TransactionAmt ProductCD  ...  id_32    id_33  DeviceInfo\n",
       "316078        7864752         107.950         W  ...   24.0  unknown     unknown\n",
       "116888        2260318          37.021         C  ...   24.0  unknown     Windows\n",
       "410211       10361225          59.000         W  ...   24.0  unknown     unknown\n",
       "251027        5974846          57.950         W  ...   24.0  unknown     unknown\n",
       "191265        4296816         107.950         W  ...   24.0  unknown     unknown\n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee2ad11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('nominal',ohe,nominal_cols),\n",
    "    ('ordinal',oe,ordinal_cols),\n",
    "    ('freq',FrequencyEncoder(),nominal_cols_high),\n",
    "    ('numeric',scaler,numeric_cols)\n",
    "],remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba11da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_df.drop(columns=['isFraud'])\n",
    "y_train=train_df['isFraud']\n",
    "X_test=test_df.drop(columns=['isFraud'])\n",
    "y_test=test_df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f538bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed=preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed=preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "539c2930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118108, 140)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b0c138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(preprocessor):\n",
    "    feature_names=[]\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "            else:\n",
    "                names = columns\n",
    "            feature_names.extend(names)\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6760a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans=pd.DataFrame(X_train_preprocessed,columns=get_feature_names(preprocessor))\n",
    "x_test_trans=pd.DataFrame(X_test_preprocessed,columns=get_feature_names(preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6b37f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductCD_H</th>\n",
       "      <th>ProductCD_R</th>\n",
       "      <th>ProductCD_S</th>\n",
       "      <th>ProductCD_W</th>\n",
       "      <th>card4_discover</th>\n",
       "      <th>card4_mastercard</th>\n",
       "      <th>card4_unknown</th>\n",
       "      <th>card4_visa</th>\n",
       "      <th>card6_credit</th>\n",
       "      <th>card6_debit</th>\n",
       "      <th>...</th>\n",
       "      <th>id_04</th>\n",
       "      <th>id_05</th>\n",
       "      <th>id_06</th>\n",
       "      <th>id_09</th>\n",
       "      <th>id_13</th>\n",
       "      <th>id_14</th>\n",
       "      <th>id_17</th>\n",
       "      <th>id_19</th>\n",
       "      <th>id_20</th>\n",
       "      <th>id_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-0.154705</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>-0.036239</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>0.167209</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>0.214857</td>\n",
       "      <td>-0.217027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-0.154705</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>-0.036239</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>0.167209</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>0.214857</td>\n",
       "      <td>-0.217027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-0.154705</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>-0.036239</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>0.167209</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>0.214857</td>\n",
       "      <td>-0.217027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-0.154705</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>-0.036239</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>0.167209</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>-0.037241</td>\n",
       "      <td>0.214857</td>\n",
       "      <td>-0.217027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028753</td>\n",
       "      <td>-0.154705</td>\n",
       "      <td>0.186745</td>\n",
       "      <td>-0.036239</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>-4.397336</td>\n",
       "      <td>-0.312222</td>\n",
       "      <td>2.848053</td>\n",
       "      <td>-3.836656</td>\n",
       "      <td>4.562278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductCD_H  ProductCD_R  ProductCD_S  ProductCD_W  card4_discover  \\\n",
       "0          0.0          0.0          0.0          1.0             1.0   \n",
       "1          0.0          0.0          0.0          1.0             0.0   \n",
       "2          0.0          0.0          0.0          1.0             0.0   \n",
       "3          0.0          0.0          0.0          1.0             0.0   \n",
       "4          1.0          0.0          0.0          0.0             0.0   \n",
       "\n",
       "   card4_mastercard  card4_unknown  card4_visa  card6_credit  card6_debit  \\\n",
       "0               0.0            0.0         0.0           1.0          0.0   \n",
       "1               1.0            0.0         0.0           1.0          0.0   \n",
       "2               0.0            0.0         1.0           0.0          1.0   \n",
       "3               1.0            0.0         0.0           0.0          1.0   \n",
       "4               1.0            0.0         0.0           1.0          0.0   \n",
       "\n",
       "   ...     id_04     id_05     id_06     id_09     id_13     id_14     id_17  \\\n",
       "0  ...  0.028753 -0.154705  0.186745 -0.036239  0.179111  0.167209 -0.312222   \n",
       "1  ...  0.028753 -0.154705  0.186745 -0.036239  0.179111  0.167209 -0.312222   \n",
       "2  ...  0.028753 -0.154705  0.186745 -0.036239  0.179111  0.167209 -0.312222   \n",
       "3  ...  0.028753 -0.154705  0.186745 -0.036239  0.179111  0.167209 -0.312222   \n",
       "4  ...  0.028753 -0.154705  0.186745 -0.036239  0.179111 -4.397336 -0.312222   \n",
       "\n",
       "      id_19     id_20     id_32  \n",
       "0 -0.037241  0.214857 -0.217027  \n",
       "1 -0.037241  0.214857 -0.217027  \n",
       "2 -0.037241  0.214857 -0.217027  \n",
       "3 -0.037241  0.214857 -0.217027  \n",
       "4  2.848053 -3.836656  4.562278  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_trans.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab17feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=0.90,random_state=42)\n",
    "x_train_pca=pca.fit_transform(x_train_trans)\n",
    "x_test_pca=pca.transform(x_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8b79a488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isinf(X_train_preprocessed).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "281e1140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16530, number of negative: 455902\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17595\n",
      "[LightGBM] [Info] Number of data points in the train set: 472432, number of used features: 69\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034989 -> initscore=-3.317101\n",
      "[LightGBM] [Info] Start training from score -3.317101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113975\n",
      "           1       0.82      0.30      0.44      4133\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.90      0.65      0.71    118108\n",
      "weighted avg       0.97      0.97      0.97    118108\n",
      "\n",
      "[[113710    265]\n",
      " [  2891   1242]]\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model=LGBMClassifier()\n",
    "model.fit(x_train_pca,y_train)\n",
    "y_pred=model.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f33f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_cols=[f\"[ca_{i}]\" for i in range(x_train_pca.shape[1])]\n",
    "df_aug=pd.DataFrame(x_train_pca, columns=pca_cols)\n",
    "df_aug['isFraud']=y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95    114044\n",
      "           1       0.20      0.58      0.30      4064\n",
      "\n",
      "    accuracy                           0.91    118108\n",
      "   macro avg       0.59      0.75      0.63    118108\n",
      "weighted avg       0.96      0.91      0.93    118108\n",
      "\n",
      "[[104800   9244]\n",
      " [  1717   2347]]\n"
     ]
    }
   ],
   "source": [
    "mh_aug=MHSysGen(method=\"parallel\",ratio=15,minority_class=1)\n",
    "X_aug,y_aug=mh_aug.fit_resample(df_aug,target=\"isFraud\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=600,\n",
    "                               max_depth=18,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=2,\n",
    "                               max_features=\"sqrt\",\n",
    "                               class_weight='balanced',\n",
    "                               random_state=42,\n",
    "                               )\n",
    "model.fit(X_aug,y_aug)\n",
    "y_pred=model.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e37eb192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113975\n",
      "           1       0.85      0.51      0.63      4133\n",
      "\n",
      "    accuracy                           0.98    118108\n",
      "   macro avg       0.91      0.75      0.81    118108\n",
      "weighted avg       0.98      0.98      0.98    118108\n",
      "\n",
      "[[113594    381]\n",
      " [  2040   2093]]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote=SMOTE(sampling_strategy=0.2,random_state=42)\n",
    "X_smote,y_smote=smote.fit_resample(x_train_pca,y_train)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_smote,y_smote)\n",
    "y_pred=model.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb83c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95    114044\n",
      "           1       0.20      0.58      0.30      4064\n",
      "\n",
      "    accuracy                           0.91    118108\n",
      "   macro avg       0.59      0.75      0.63    118108\n",
      "weighted avg       0.96      0.91      0.93    118108\n",
      "\n",
      "[[104800   9244]\n",
      " [  1717   2347]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class PWMv5Fast:\n",
    "    def __init__(self, ratio=20, random_state=42):\n",
    "        self.ratio = ratio\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        min_class = classes[np.argmin(counts)]\n",
    "        X_min = X[y == min_class]\n",
    "        X_maj = X[y != min_class]\n",
    "\n",
    "        # Precompute covariance noise\n",
    "        cov = np.cov(X_min.T)\n",
    "        L = np.linalg.cholesky(cov + np.eye(cov.shape[0])*1e-6)\n",
    "\n",
    "        # Build KDTree only once\n",
    "        tree = KDTree(X_maj)\n",
    "\n",
    "        n_new = len(X_min) * self.ratio\n",
    "\n",
    "        # 1) Compute majority outward direction ONCE for each minority point\n",
    "        dists, idxs = tree.query(X_min, k=10)\n",
    "        maj_centers = np.array([X_maj[idx].mean(axis=0) for idx in idxs])\n",
    "        outward = X_min - maj_centers\n",
    "        outward /= np.linalg.norm(outward, axis=1, keepdims=True) + 1e-9\n",
    "\n",
    "        # 2) Sample synthetic points in a vectorized form\n",
    "        choices = np.random.randint(0, len(X_min), size=n_new)\n",
    "        base = X_min[choices]\n",
    "        out = outward[choices]\n",
    "\n",
    "        # scaling\n",
    "        scale = np.random.uniform(1.0, 2.0, size=n_new).reshape(-1, 1)\n",
    "\n",
    "        # random walk vector, normalized\n",
    "        rand = np.random.randn(n_new, X.shape[1])\n",
    "        rand /= np.linalg.norm(rand, axis=1, keepdims=True) + 1e-9\n",
    "\n",
    "        # covariance noise in vectorized manner\n",
    "        noise = np.dot(np.random.randn(n_new, X.shape[1]), L.T) * 0.03\n",
    "\n",
    "        # final synthetic points\n",
    "        synth = base + out * scale + rand * 0.5 + noise\n",
    "\n",
    "        X_new = np.vstack([X, synth])\n",
    "        y_new = np.concatenate([y, np.full(n_new, min_class)])\n",
    "\n",
    "        return X_new, y_new\n",
    "\n",
    "\n",
    "\n",
    "p5 = PWMv5Fast(ratio=20)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=600,\n",
    "                               max_depth=18,\n",
    "                               min_samples_split=5,\n",
    "                               min_samples_leaf=2,\n",
    "                               max_features=\"sqrt\",\n",
    "                               class_weight='balanced',\n",
    "                               random_state=42,\n",
    "                               \n",
    "                               )\n",
    "X5, y5 = p5.fit_resample(x_train_pca, y_train)\n",
    "model.fit(X5,y5)\n",
    "y_pred=model.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9a07163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    113975\n",
      "           1       0.94      0.37      0.53      4133\n",
      "\n",
      "    accuracy                           0.98    118108\n",
      "   macro avg       0.96      0.68      0.76    118108\n",
      "weighted avg       0.98      0.98      0.97    118108\n",
      "\n",
      "[[113875    100]\n",
      " [  2606   1527]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train_pca,y_train)\n",
    "y_pred=model.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "started dividing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [13:41:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    114044\n",
      "           1       0.72      0.32      0.44      4064\n",
      "\n",
      "    accuracy                           0.97    118108\n",
      "   macro avg       0.85      0.66      0.71    118108\n",
      "weighted avg       0.97      0.97      0.97    118108\n",
      "\n",
      "[[113534    510]\n",
      " [  2774   1290]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mh_sys_gen import MHSysGen\n",
    "from sklearn.model_selection import train_test_split\n",
    "df=pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"],inplace=True)\n",
    "categorical_feature=[]\n",
    "for col in df.columns:\n",
    "    if df[col].dtype==\"object\":\n",
    "        categorical_feature.append(col)\n",
    "nominal_cols=['ProductCD','card4','card6']#low cardinality categorical features\n",
    "nominal_cols_high=['P_emaildomain','R_emaildomain',\"id_30\",\"id_31\",\"DeviceInfo\",\"id_33\"]#high cardinality categorical features \n",
    "ordinal_cols=['M4','M2','M3','M5','M6']\n",
    "numeric_cols=[]\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols_high and col not in nominal_cols and col not in ordinal_cols and df[col].dtype!=\"object\":\n",
    "        if col!=\"isFraud\":\n",
    "         numeric_cols.append(col)\n",
    "df=df.sort_values(by=\"TransactionDT\")\n",
    "split_index=int(0.8*len(df))\n",
    "train_df = df.iloc[:split_index]\n",
    "test_df = df.iloc[split_index:]\n",
    "#frequency encoding \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def init(self,freq_maps=None):\n",
    "        self.freq_maps=freq_maps\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        X=pd.DataFrame(X)\n",
    "        self.freq_maps={}\n",
    "        for col in X.columns:\n",
    "            freq_map=X[col].value_counts(normalize=True)\n",
    "            self.freq_maps[col]=freq_map\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X=pd.DataFrame(X)\n",
    "        X_transformed=X.copy()\n",
    "        for col in X.columns:\n",
    "            X_transformed[col]=X_transformed[col].map(self.freq_maps[col]).fillna(0)\n",
    "        return X_transformed\n",
    "from sklearn.preprocessing import OneHotEncoder ,OrdinalEncoder,StandardScaler\n",
    "ohe=OneHotEncoder(drop='first',sparse_output=False,handle_unknown='ignore')\n",
    "oe=OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1)\n",
    "scaler=StandardScaler()\n",
    "df[nominal_cols_high].fillna('missing')\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor=ColumnTransformer(transformers=[\n",
    "    ('nominal',ohe,nominal_cols),\n",
    "    ('ordinal',oe,ordinal_cols),\n",
    "    ('freq',FrequencyEncoder(),nominal_cols_high),\n",
    "    ('numeric',scaler,numeric_cols)\n",
    "],remainder='passthrough')\n",
    "X_train=train_df.drop(columns=['isFraud'])\n",
    "y_train=train_df['isFraud']\n",
    "X_test=test_df.drop(columns=['isFraud'])\n",
    "y_test=test_df['isFraud']\n",
    "X_train_preprocessed=preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed=preprocessor.transform(X_test)\n",
    "def get_feature_names(preprocessor):\n",
    "    feature_names=[]\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "            else:\n",
    "                names = columns\n",
    "            feature_names.extend(names)\n",
    "    return feature_names\n",
    "x_train_trans=pd.DataFrame(X_train_preprocessed,columns=get_feature_names(preprocessor))\n",
    "x_test_trans=pd.DataFrame(X_test_preprocessed,columns=get_feature_names(preprocessor))\n",
    "print(\"started\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=0.90,random_state=42)\n",
    "x_train_pca=pca.fit_transform(x_train_trans)\n",
    "x_test_pca=pca.transform(x_test_trans)\n",
    "\n",
    "\n",
    "print(\"started dividing\")\n",
    "\n",
    "df_raw=x_train_trans.copy()\n",
    "df_raw['isFraud']=y_train.values\n",
    "mh_aug=MHSysGen(method=\"parallel\",ratio=15,minority_class=1)\n",
    "print(\"started aug\")\n",
    "X_aug,y_aug=mh_aug.fit_resample(df_raw,target=\"isFraud\")\n",
    "x_aug_pca=pca.transform(X_aug)\n",
    "y_aug_pca=y_aug\n",
    "x_aug_pca=pca.transform(X_smote)\n",
    "y_aug_pca=y_smote\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "model_mhsysgen=XGBClassifier(n_estimators=600,\n",
    "                               max_depth=18,\n",
    "                               min_child_weight=5,\n",
    "                               learning_rate=0.1,\n",
    "                               subsample=0.8,\n",
    "                               colsample_bytree=0.8,\n",
    "                               scale_pos_weight=1,\n",
    "                               random_state=42,\n",
    "                               use_label_encoder=False,\n",
    "                               eval_metric='logloss'\n",
    "                               )\n",
    "model_mhsysgen.fit(x_aug_pca,y_aug_pca)\n",
    "y_pred=model_mhsysgen.predict(x_test_pca)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b2128",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (2429730967.py, line 172)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(confusion_matrix(y_test, y_pred))\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mh_sys_gen import MHSysGen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "numeric_cols = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "       col not in nominal_high and \\\n",
    "       col not in ordinal_cols and \\\n",
    "       col != \"isFraud\" and \\\n",
    "       df[col].dtype != \"object\":\n",
    "        numeric_cols.append(col)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def _init_(self):\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            self.freq_maps[col] = X[col].value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            X2[col] = X2[col].map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        (\"freq\", freq, nominal_high),\n",
    "        (\"num\", scaler, numeric_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 140)\n",
      "PCA completed. Train PCA shape: (472432, 58)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 140)\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [14:26:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL RESULTS (MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114044\n",
      "           1       0.45      0.47      0.46      4064\n",
      "\n",
      "    accuracy                           0.96    118108\n",
      "   macro avg       0.72      0.72      0.72    118108\n",
      "weighted avg       0.96      0.96      0.96    118108\n",
      "\n",
      "[[111723   2321]\n",
      " [  2166   1898]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mh_sys_gen import MHSysGen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "numeric_cols = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "       col not in nominal_high and \\\n",
    "       col not in ordinal_cols and \\\n",
    "       col != \"isFraud\" and \\\n",
    "       df[col].dtype != \"object\":\n",
    "        numeric_cols.append(col)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            self.freq_maps[col] = X[col].value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            X2[col] = X2[col].map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        (\"freq\", freq, nominal_high),\n",
    "        (\"num\", scaler, numeric_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fdda0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 79)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 235\u001b[39m\n\u001b[32m    232\u001b[39m mh = MHSysGen(method=\u001b[33m\"\u001b[39m\u001b[33mparallel\u001b[39m\u001b[33m\"\u001b[39m, ratio=\u001b[32m15\u001b[39m, minority_class=\u001b[32m1\u001b[39m)\n\u001b[32m    234\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting MH-SysGen augmentation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m X_aug, y_aug = \u001b[43mmh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43misFraud\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAugmentation complete. New shape:\u001b[39m\u001b[33m\"\u001b[39m, X_aug.shape)\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# PCA transform on augmented data (NO FIT AGAIN!)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\mh_sys_gen\\core.py:10\u001b[39m, in \u001b[36mMHSysGen.fit_resample\u001b[39m\u001b[34m(self, df, target)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33mparallel\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhybrid_parallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hybrid_augmentation\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhybrid_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mminority_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33msequential\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhybrid_sequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequential_shadow_mirror\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\mh_sys_gen\\hybrid_parallel.py:19\u001b[39m, in \u001b[36mhybrid_augmentation\u001b[39m\u001b[34m(df, target_col, minority_class, ratio)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_synthetic < \u001b[32m1\u001b[39m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRatio too small — produces zero synthetic samples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df_mirror = \u001b[43mmirror_neighbor_aug_with_shape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminority_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mminority_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_synthetic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_synthetic\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m df_shadow = shadow_augmentation(\n\u001b[32m     28\u001b[39m     df=df,\n\u001b[32m     29\u001b[39m     target_col=target_col,\n\u001b[32m     30\u001b[39m     minority_class=minority_class,\n\u001b[32m     31\u001b[39m     total_synthetic=total_synthetic\n\u001b[32m     32\u001b[39m )\n\u001b[32m     35\u001b[39m synthetic_df = pd.concat([df_mirror, df_shadow], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\mh_sys_gen\\mirror.py:22\u001b[39m, in \u001b[36mmirror_neighbor_aug_with_shape\u001b[39m\u001b[34m(df, target_col, minority_class, total_synthetic, n_neighbors, shape_scale_range)\u001b[39m\n\u001b[32m     19\u001b[39m idx = random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(minority_df)-\u001b[32m1\u001b[39m)\n\u001b[32m     20\u001b[39m original = X[idx]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m _, indices = \u001b[43mnbrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m neighbor = X[random.choice(indices[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m:])]\n\u001b[32m     25\u001b[39m direction = neighbor - original\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\threadpoolctl.py:921\u001b[39m, in \u001b[36mThreadpoolController.limit\u001b[39m\u001b[34m(self, limits, user_api)\u001b[39m\n\u001b[32m    870\u001b[39m \u001b[38;5;129m@_format_docstring\u001b[39m(\n\u001b[32m    871\u001b[39m     USER_APIS=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m.format(api) \u001b[38;5;28;01mfor\u001b[39;00m api \u001b[38;5;129;01min\u001b[39;00m _ALL_USER_APIS),\n\u001b[32m    872\u001b[39m     BLAS_LIBS=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(_ALL_BLAS_LIBRARIES),\n\u001b[32m    873\u001b[39m     OPENMP_LIBS=\u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(_ALL_OPENMP_LIBRARIES),\n\u001b[32m    874\u001b[39m )\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlimit\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, limits=\u001b[38;5;28;01mNone\u001b[39;00m, user_api=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    876\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Change the maximal number of threads that can be used in thread pools.\u001b[39;00m\n\u001b[32m    877\u001b[39m \n\u001b[32m    878\u001b[39m \u001b[33;03m    This function returns an object that can be used either as a callable (the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m \u001b[33;03m        - If None, this function will apply to all supported libraries.\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ThreadpoolLimiter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\threadpoolctl.py:587\u001b[39m, in \u001b[36m_ThreadpoolLimiter.__init__\u001b[39m\u001b[34m(self, controller, limits, user_api)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m._limits, \u001b[38;5;28mself\u001b[39m._user_api, \u001b[38;5;28mself\u001b[39m._prefixes = \u001b[38;5;28mself\u001b[39m._check_params(\n\u001b[32m    584\u001b[39m     limits, user_api\n\u001b[32m    585\u001b[39m )\n\u001b[32m    586\u001b[39m \u001b[38;5;28mself\u001b[39m._original_info = \u001b[38;5;28mself\u001b[39m._controller.info()\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\threadpoolctl.py:720\u001b[39m, in \u001b[36m_ThreadpoolLimiter._set_threadpool_limits\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[43mlib_controller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_num_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\threadpoolctl.py:199\u001b[39m, in \u001b[36mOpenBLASController.set_num_threads\u001b[39m\u001b[34m(self, num_threads)\u001b[39m\n\u001b[32m    197\u001b[39m set_num_threads_func = \u001b[38;5;28mself\u001b[39m._get_symbol(\u001b[33m\"\u001b[39m\u001b[33mopenblas_set_num_threads\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m set_num_threads_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mset_num_threads_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    # use_label_encoder=False is deprecated and removed in recent XGBoost versions.\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1723f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 79)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 150)\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114044\n",
      "           1       0.42      0.50      0.46      4064\n",
      "\n",
      "    accuracy                           0.96    118108\n",
      "   macro avg       0.70      0.74      0.72    118108\n",
      "weighted avg       0.96      0.96      0.96    118108\n",
      "\n",
      "[[111258   2786]\n",
      " [  2043   2021]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "\n",
    "class FullFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    FULL FEATURE ENGINEERING for IEEE-CIS Fraud Dataset.\n",
    "    Includes:\n",
    "    - Time features\n",
    "    - Email grouping\n",
    "    - Device parsing\n",
    "    - id_33 resolution split\n",
    "    - Amount log\n",
    "    - Card + Addr aggregations\n",
    "    - Ratio features\n",
    "    - Cyclic features\n",
    "    \"\"\"\n",
    "\n",
    "    def _init_(self, \n",
    "                 time_col='TransactionDT', \n",
    "                 amt_col='TransactionAmt'):\n",
    "        \n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        \n",
    "        # saved mappings\n",
    "        self.email_map = {}\n",
    "        self.device_brand_map = {}\n",
    "        self.agg_features = {}\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X['isFraud'] = y\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1. EMAIL GROUP MAPPING\n",
    "        # -----------------------------\n",
    "        def map_email(x):\n",
    "            x = str(x).lower()\n",
    "            if \"gmail\" in x: return \"gmail\"\n",
    "            if \"outlook\" in x or \"hotmail\" in x or \"live\" in x: return \"microsoft\"\n",
    "            if \"yahoo\" in x: return \"yahoo\"\n",
    "            if \"icloud\" in x or \"mac\" in x or \"apple\" in x: return \"apple\"\n",
    "            if x == \"nan\": return \"unknown\"\n",
    "            return \"other\"\n",
    "\n",
    "        email_cols = [\"P_emaildomain\", \"R_emaildomain\"]\n",
    "        for col in email_cols:\n",
    "            self.email_map[col] = X[col].astype(str).apply(map_email)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2. DEVICE BRAND MAPPING\n",
    "        # -----------------------------\n",
    "        def parse_device(x):\n",
    "            x = str(x).lower()\n",
    "            if \"sm\" in x: return \"samsung\"\n",
    "            if \"moto\" in x: return \"motorola\"\n",
    "            if \"iphone\" in x: return \"iphone\"\n",
    "            if \"mac\" in x: return \"apple\"\n",
    "            if \"huawei\" in x: return \"huawei\"\n",
    "            if \"lg\" in x: return \"lg\"\n",
    "            if x == \"nan\": return \"unknown\"\n",
    "            return \"other\"\n",
    "\n",
    "        self.device_brand_map = X[\"DeviceInfo\"].astype(str).apply(parse_device)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3. AGGREGATION FEATURES\n",
    "        # -----------------------------\n",
    "        for col in self.base_agg_keys:\n",
    "            # count / frequency\n",
    "            self.agg_features[f\"{col}_Count_Map\"] = X[col].value_counts()\n",
    "\n",
    "            # amount mean\n",
    "            self.agg_features[f\"{col}_Amt_Mean_Map\"] = X.groupby(col)[self.amt_col].mean()\n",
    "\n",
    "            # amount std\n",
    "            self.agg_features[f\"{col}_Amt_Std_Map\"] = (\n",
    "                X.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # -----------------------------\n",
    "        # EMAIL GROUPING\n",
    "        # -----------------------------\n",
    "        def map_email(x):\n",
    "            x = str(x).lower()\n",
    "            if \"gmail\" in x: return \"gmail\"\n",
    "            if \"outlook\" in x or \"hotmail\" in x or \"live\" in x: return \"microsoft\"\n",
    "            if \"yahoo\" in x: return \"yahoo\"\n",
    "            if \"icloud\" in x or \"mac\" in x or \"apple\" in x: return \"apple\"\n",
    "            if x == \"nan\": return \"unknown\"\n",
    "            return \"other\"\n",
    "\n",
    "        X[\"P_email_group\"] = X[\"P_emaildomain\"].astype(str).apply(map_email)\n",
    "        X[\"R_email_group\"] = X[\"R_emaildomain\"].astype(str).apply(map_email)\n",
    "\n",
    "        # -----------------------------\n",
    "        # DEVICE BRAND\n",
    "        # -----------------------------\n",
    "        def parse_device(x):\n",
    "            x = str(x).lower()\n",
    "            if \"sm\" in x: return \"samsung\"\n",
    "            if \"moto\" in x: return \"motorola\"\n",
    "            if \"iphone\" in x: return \"iphone\"\n",
    "            if \"mac\" in x: return \"apple\"\n",
    "            if \"huawei\" in x: return \"huawei\"\n",
    "            if \"lg\" in x: return \"lg\"\n",
    "            return \"other\"\n",
    "\n",
    "        X[\"Device_brand\"] = X[\"DeviceInfo\"].astype(str).apply(parse_device)\n",
    "\n",
    "        # -----------------------------\n",
    "        # ID_33 RESOLUTION SPLIT\n",
    "        # -----------------------------\n",
    "        X[\"id_33_h\"] = X[\"id_33\"].astype(str).str.split(\"x\").str[0].astype(float)\n",
    "        X[\"id_33_w\"] = X[\"id_33\"].astype(str).str.split(\"x\").str[1].astype(float)\n",
    "\n",
    "        # -----------------------------\n",
    "        # TIME FEATURES\n",
    "        # -----------------------------\n",
    "        if self.time_col in X.columns:\n",
    "            X[\"hour\"] = (X[self.time_col] // 3600) % 24\n",
    "            X[\"day\"] = (X[self.time_col] // (3600 * 24))\n",
    "            X[\"weekday\"] = X[\"day\"] % 7\n",
    "            X[\"is_weekend\"] = (X[\"weekday\"] >= 5).astype(int)\n",
    "\n",
    "            # cyclic features\n",
    "            X[\"hour_sin\"] = np.sin(2 * np.pi * X[\"hour\"] / 24)\n",
    "            X[\"hour_cos\"] = np.cos(2 * np.pi * X[\"hour\"] / 24)\n",
    "\n",
    "        # -----------------------------\n",
    "        # AMOUNT FEATURES\n",
    "        # -----------------------------\n",
    "        X[\"amt_log\"] = np.log1p(X[self.amt_col])\n",
    "\n",
    "        # -----------------------------\n",
    "        # AGGREGATE FEATURES\n",
    "        # -----------------------------\n",
    "        for col in self.base_agg_keys:\n",
    "            X[f\"{col}_Count\"] = X[col].map(self.agg_features[f\"{col}_Count_Map\"]).fillna(0)\n",
    "            X[f\"{col}_Amt_Mean\"] = X[col].map(self.agg_features[f\"{col}_Amt_Mean_Map\"]).fillna(X[self.amt_col].mean())\n",
    "            X[f\"{col}_Amt_Std\"] = X[col].map(self.agg_features[f\"{col}_Amt_Std_Map\"]).fillna(1.0)\n",
    "\n",
    "        # ratio feature\n",
    "        X[\"Amt_over_card1_mean\"] = X[self.amt_col] / (X[\"card1_Amt_Mean\"] + 1)\n",
    "\n",
    "        return X\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    # use_label_encoder=False is deprecated and removed in recent XGBoost versions.\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24cb4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 79)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 150)\n",
      "\n",
      "Training XGBoost...\n",
      "0:\tlearn: 0.9713755\ttotal: 818ms\tremaining: 20m 25s\n",
      "200:\tlearn: 0.9795176\ttotal: 2m 21s\tremaining: 15m 11s\n",
      "400:\tlearn: 0.9842915\ttotal: 4m 41s\tremaining: 12m 51s\n",
      "600:\tlearn: 0.9873482\ttotal: 6m 49s\tremaining: 10m 12s\n",
      "800:\tlearn: 0.9895507\ttotal: 8m 32s\tremaining: 7m 27s\n",
      "1000:\tlearn: 0.9913690\ttotal: 10m 19s\tremaining: 5m 8s\n",
      "1200:\tlearn: 0.9928230\ttotal: 12m 5s\tremaining: 3m\n",
      "1400:\tlearn: 0.9940061\ttotal: 13m 49s\tremaining: 58.6s\n",
      "1499:\tlearn: 0.9944945\ttotal: 14m 42s\tremaining: 0us\n",
      "\n",
      "===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.78      0.87    114044\n",
      "           1       0.11      0.79      0.20      4064\n",
      "\n",
      "    accuracy                           0.78    118108\n",
      "   macro avg       0.55      0.79      0.54    118108\n",
      "weighted avg       0.96      0.78      0.85    118108\n",
      "\n",
      "[[89058 24986]\n",
      " [  835  3229]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1500,\n",
    "                           depth=10,\n",
    "                           learning_rate=0.05,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           scale_pos_weight=15,\n",
    "                           random_seed=42,\n",
    "                           verbose=200\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ded9a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 79)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (804412, 150)\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114044\n",
      "           1       0.43      0.50      0.46      4064\n",
      "\n",
      "    accuracy                           0.96    118108\n",
      "   macro avg       0.71      0.74      0.72    118108\n",
      "weighted avg       0.96      0.96      0.96    118108\n",
      "\n",
      "[[111396   2648]\n",
      " [  2031   2033]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class FastSequentialShadowMirror:\n",
    "    \"\"\"\n",
    "    FAST version of Sequential Shadow-Mirror oversampling.\n",
    "    Same ideology:\n",
    "      1. Shadow scaling\n",
    "      2. Mirror using nearest neighbor reflection\n",
    "      3. Vectorized & optimized (10-40x faster)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_col, minority_class=1, ratio=1.0):\n",
    "        self.target_col = target_col\n",
    "        self.minority_class = minority_class\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def fit_resample(self, df):\n",
    "        \"\"\"\n",
    "        df : DataFrame containing both features and target_col  \n",
    "        returns: X_resampled, y_resampled\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # PREPARE DATA\n",
    "        # ----------------------------\n",
    "        features = [c for c in df.columns if c != self.target_col]\n",
    "        minority_df = df[df[self.target_col] == self.minority_class]\n",
    "\n",
    "        if minority_df.empty:\n",
    "            raise ValueError(f\"Minority class {self.minority_class} not found.\")\n",
    "\n",
    "        total_synth = int(len(minority_df) * self.ratio)\n",
    "        if total_synth < 1:\n",
    "            raise ValueError(\"Ratio too small; produced 0 samples.\")\n",
    "\n",
    "        X_min = minority_df[features].values\n",
    "        n_features = X_min.shape[1]\n",
    "\n",
    "        # ----------------------------\n",
    "        # PHASE 1 — SHADOW (VECTORIZED)\n",
    "        # ----------------------------\n",
    "        idxs = np.random.randint(0, len(X_min), size=total_synth)\n",
    "        originals = X_min[idxs]\n",
    "\n",
    "        scales = np.random.uniform(0.6, 1.4, size=(total_synth, n_features))\n",
    "        shadows = originals * scales\n",
    "\n",
    "        # ----------------------------\n",
    "        # PHASE 2 — MIRROR (VECTORIZED KNN)\n",
    "        # ----------------------------\n",
    "        nbrs = NearestNeighbors(n_neighbors=5).fit(shadows)\n",
    "        _, idx_arrays = nbrs.kneighbors(shadows)\n",
    "\n",
    "        neighbors = shadows[idx_arrays[:, 1]]\n",
    "\n",
    "        reflect_scale = np.random.uniform(0.5, 1.3, size=(total_synth, 1))\n",
    "        mirrored = shadows + (neighbors - shadows) * reflect_scale\n",
    "\n",
    "        # ----------------------------\n",
    "        # BUILD SYNTHETIC DF\n",
    "        # ----------------------------\n",
    "        synthetic_df = pd.DataFrame(mirrored, columns=features)\n",
    "        synthetic_df[self.target_col] = self.minority_class\n",
    "\n",
    "        combined = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "        X_out = combined.drop(columns=[self.target_col])\n",
    "        y_out = combined[self.target_col]\n",
    "\n",
    "        return X_out, y_out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "sm=FastSequentialShadowMirror(target_col=\"isFraud\",minority_class=1,ratio=20)\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = sm.fit_resample(df_aug)\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "from catboost import CatBoostClassifier\n",
    "model = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    # use_label_encoder=False is deprecated and removed in recent XGBoost versions.\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "model.fit(X_aug_pca, y_aug)\n",
    "\n",
    "y_pred = model.predict(x_test_pca)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b509f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 63)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 150)\n",
      "Training catboost\n",
      "0:\tlearn: 0.9714687\ttotal: 400ms\tremaining: 9m 59s\n",
      "200:\tlearn: 0.9792732\ttotal: 1m 27s\tremaining: 9m 25s\n",
      "400:\tlearn: 0.9838024\ttotal: 2m 56s\tremaining: 8m 3s\n",
      "600:\tlearn: 0.9868530\ttotal: 4m 25s\tremaining: 6m 37s\n",
      "800:\tlearn: 0.9890632\ttotal: 5m 53s\tremaining: 5m 8s\n",
      "1000:\tlearn: 0.9908265\ttotal: 7m 22s\tremaining: 3m 40s\n",
      "1200:\tlearn: 0.9922523\ttotal: 8m 51s\tremaining: 2m 12s\n",
      "1400:\tlearn: 0.9934665\ttotal: 10m 21s\tremaining: 43.9s\n",
      "1499:\tlearn: 0.9939615\ttotal: 11m 5s\tremaining: 0us\n",
      "training xgboost\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "from catboost import CatBoostClassifier\n",
    "model_cat = CatBoostClassifier(iterations=1500,\n",
    "                           depth=10,\n",
    "                           learning_rate=0.05,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           scale_pos_weight=15,\n",
    "                           random_seed=42,\n",
    "                           verbose=200\n",
    "    \n",
    ")\n",
    "print(\"Training catboost\")\n",
    "model_cat.fit(X_aug_pca, y_aug)\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=600,\n",
    "    max_depth=18,\n",
    "    min_child_weight=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    random_state=42,\n",
    "    # use_label_encoder=False is deprecated and removed in recent XGBoost versions.\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "print(\"training xgboost\")\n",
    "model_xgb.fit(X_aug_pca, y_aug)\n",
    "\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8965d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114044\n",
      "           1       0.48      0.47      0.48      4064\n",
      "\n",
      "    accuracy                           0.96    118108\n",
      "   macro avg       0.73      0.73      0.73    118108\n",
      "weighted avg       0.96      0.96      0.96    118108\n",
      "\n",
      "[[111993   2051]\n",
      " [  2152   1912]]\n",
      "roc-auc score 0.7262440770891854\n"
     ]
    }
   ],
   "source": [
    "p_xgb_train=model_xgb.predict_proba(x_train_pca)[:,1]\n",
    "p_cat_train=model_cat.predict_proba(x_train_pca)[:,1]\n",
    "stack_train=np.column_stack([p_xgb_train,p_cat_train])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "meta_model=LogisticRegression(max_iter=1000)\n",
    "meta_model.fit(stack_train,y_train)\n",
    "\n",
    "p_xgb_test=model_xgb.predict_proba(x_test_pca)[:,1]\n",
    "p_cat_test=model_cat.predict_proba(x_test_pca)[:,1]\n",
    "stack_test=np.column_stack([p_xgb_test,p_cat_test])\n",
    "\n",
    "proba=meta_model.predict_proba(stack_test)[:,1]\n",
    "final_pred=(proba >=0.27).astype(int)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, final_pred))\n",
    "print(confusion_matrix(y_test,final_pred))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "rc_score=roc_auc_score(y_test, final_pred)\n",
    "print(\"roc-auc score\",rc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1498398c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 79)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 14:46:42,918] A new study created in memory with name: no-name-4c7a9bbb-a3d0-4e44-991f-17ec6f4579c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Optuna tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 14:48:43,470] Trial 0 finished with value: 0.885928767486689 and parameters: {'n_estimators': 372, 'max_depth': 15, 'min_child_weight': 9, 'learning_rate': 0.04260807738692856, 'subsample': 0.6322739922792978, 'colsample_bytree': 0.789378175020648, 'gamma': 0.5473581155773255, 'reg_lambda': 0.0019094698638261935, 'reg_alpha': 0.16454201700646068}. Best is trial 0 with value: 0.885928767486689.\n",
      "[I 2025-12-09 14:49:59,773] Trial 1 finished with value: 0.8743644865053466 and parameters: {'n_estimators': 406, 'max_depth': 10, 'min_child_weight': 5, 'learning_rate': 0.035339559893218984, 'subsample': 0.9008341499754189, 'colsample_bytree': 0.7553248133039082, 'gamma': 1.8510517324939242, 'reg_lambda': 0.20117386869376658, 'reg_alpha': 1.002212647746794}. Best is trial 0 with value: 0.885928767486689.\n",
      "[I 2025-12-09 14:50:34,608] Trial 2 finished with value: 0.8529619989104219 and parameters: {'n_estimators': 343, 'max_depth': 5, 'min_child_weight': 7, 'learning_rate': 0.030247698141705543, 'subsample': 0.8732616384043383, 'colsample_bytree': 0.6421955391550745, 'gamma': 1.2162588937061618, 'reg_lambda': 0.427933725111623, 'reg_alpha': 0.010551780221188497}. Best is trial 0 with value: 0.885928767486689.\n",
      "[I 2025-12-09 14:52:09,772] Trial 3 finished with value: 0.8812449218384286 and parameters: {'n_estimators': 458, 'max_depth': 11, 'min_child_weight': 10, 'learning_rate': 0.08371522575485736, 'subsample': 0.7328615584989915, 'colsample_bytree': 0.9697175374838004, 'gamma': 0.8874147572833696, 'reg_lambda': 1.2255884093273757, 'reg_alpha': 0.0014679236026202687}. Best is trial 0 with value: 0.885928767486689.\n",
      "[I 2025-12-09 14:53:12,776] Trial 4 finished with value: 0.865965088381415 and parameters: {'n_estimators': 594, 'max_depth': 6, 'min_child_weight': 7, 'learning_rate': 0.05299269332605566, 'subsample': 0.8736256018479072, 'colsample_bytree': 0.7201956088356254, 'gamma': 4.251210245785387, 'reg_lambda': 2.5339217577732334, 'reg_alpha': 0.001880302272858019}. Best is trial 0 with value: 0.885928767486689.\n",
      "[I 2025-12-09 14:54:49,502] Trial 5 finished with value: 0.868002709342464 and parameters: {'n_estimators': 812, 'max_depth': 7, 'min_child_weight': 9, 'learning_rate': 0.029107439610719033, 'subsample': 0.899528259364405, 'colsample_bytree': 0.6810058549721791, 'gamma': 2.5036616850141518, 'reg_lambda': 1.418400243868496, 'reg_alpha': 0.3981136755129914}. Best is trial 0 with value: 0.885928767486689.\n",
      "[W 2025-12-09 14:55:06,185] Trial 6 failed with parameters: {'n_estimators': 617, 'max_depth': 17, 'min_child_weight': 2, 'learning_rate': 0.015976579383258395, 'subsample': 0.9268004876592969, 'colsample_bytree': 0.9652986473462792, 'gamma': 1.272530421834227, 'reg_lambda': 0.004639399790334111, 'reg_alpha': 0.002399661613210064} because of the following error: XGBoostError('[14:55:06] C:\\\\actions-runner\\\\_work\\\\xgboost\\\\xgboost\\\\src\\\\common\\\\io.h:362: bad_malloc: Failed to allocate 3438727168 bytes.').\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\venka\\AppData\\Local\\Temp\\ipykernel_13244\\1008537671.py\", line 267, in objective\n",
      "    model.fit(X_aug_pca, y_aug)\n",
      "    ~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1806, in fit\n",
      "    self._Booster = train(\n",
      "                    ~~~~~^\n",
      "        params,\n",
      "        ^^^^^^^\n",
      "    ...<9 lines>...\n",
      "        callbacks=self.callbacks,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\training.py\", line 199, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py\", line 2433, in update\n",
      "    _check_call(\n",
      "    ~~~~~~~~~~~^\n",
      "        _LIB.XGBoosterUpdateOneIter(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "            self.handle, ctypes.c_int(iteration), dtrain.handle\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        )\n",
      "        ^\n",
      "    )\n",
      "    ^\n",
      "  File \"f:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py\", line 323, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: [14:55:06] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:362: bad_malloc: Failed to allocate 3438727168 bytes.\n",
      "[W 2025-12-09 14:55:06,263] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[14:55:06] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:362: bad_malloc: Failed to allocate 3438727168 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting Optuna tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBest AUC from Optuna:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m    277\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, study.best_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    245\u001b[39m params = {\n\u001b[32m    246\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_int(\u001b[33m\"\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m300\u001b[39m, \u001b[32m900\u001b[39m),\n\u001b[32m    247\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: trial.suggest_int(\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m18\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscale_pos_weight\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1.0\u001b[39m,  \u001b[38;5;66;03m# you can change if needed\u001b[39;00m\n\u001b[32m    256\u001b[39m }\n\u001b[32m    258\u001b[39m model = XGBClassifier(\n\u001b[32m    259\u001b[39m     objective=\u001b[33m\"\u001b[39m\u001b[33mbinary:logistic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    260\u001b[39m     eval_metric=\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m     **params,\n\u001b[32m    265\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_aug_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_aug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m y_val_proba = model.predict_proba(x_test_pca)[:, \u001b[32m1\u001b[39m]\n\u001b[32m    269\u001b[39m auc = roc_auc_score(y_test, y_val_proba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\sklearn.py:1806\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1786\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1787\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1788\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1789\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1803\u001b[39m     feature_types=feature_types,\n\u001b[32m   1804\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1806\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1821\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py:2433\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2433\u001b[39m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\frauddetection-iee\\ieefraud\\Lib\\site-packages\\xgboost\\core.py:323\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    313\u001b[39m \n\u001b[32m    314\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [14:55:06] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\io.h:362: bad_malloc: Failed to allocate 3438727168 bytes."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 900),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 18),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"scale_pos_weight\": 1.0,  # you can change if needed\n",
    "    }\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.fit(X_aug_pca, y_aug)\n",
    "    y_val_proba = model.predict_proba(x_test_pca)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_val_proba)\n",
    "    return auc\n",
    "import optuna\n",
    "print(\"\\nStarting Optuna tuning...\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "print(\"\\nBest AUC from Optuna:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed60ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Shape: (472432, 150)\n",
      "PCA completed. Train PCA shape: (472432, 63)\n",
      "\n",
      "Starting MH-SysGen augmentation...\n",
      "Augmentation complete. New shape: (970402, 150)\n",
      "Training catboost\n",
      "0:\tlearn: 0.9721342\ttotal: 5.29s\tremaining: 2h 12m 2s\n",
      "200:\tlearn: 0.9996422\ttotal: 13m 1s\tremaining: 1h 24m 8s\n",
      "400:\tlearn: 0.9998761\ttotal: 21m 49s\tremaining: 59m 47s\n",
      "600:\tlearn: 0.9999054\ttotal: 30m 12s\tremaining: 45m 10s\n",
      "800:\tlearn: 0.9999201\ttotal: 38m 18s\tremaining: 33m 25s\n",
      "1000:\tlearn: 0.9999411\ttotal: 46m 46s\tremaining: 23m 18s\n",
      "1200:\tlearn: 0.9999585\ttotal: 55m 15s\tremaining: 13m 45s\n",
      "1400:\tlearn: 0.9999627\ttotal: 1h 3m 43s\tremaining: 4m 30s\n",
      "1499:\tlearn: 0.9999627\ttotal: 1h 7m 52s\tremaining: 0us\n",
      "training xgboost\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming mh_sys_gen is available in your environment\n",
    "from mh_sys_gen import MHSysGen \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# =========================================================\n",
    "# 0. CUSTOM FEATURE ENGINEER CLASS (NEW)\n",
    "# =========================================================\n",
    "class CustomFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to create time-based and aggregation features \n",
    "    specific to the IEEE Fraud Detection dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, time_col='TransactionDT', amt_col='TransactionAmt'):\n",
    "        self.time_col = time_col\n",
    "        self.amt_col = amt_col\n",
    "        self.agg_features = {} # Stores aggregation maps during fit\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['isFraud'] = y # Temporarily add target for aggregation/target encoding\n",
    "        \n",
    "        # 1. Base Aggregation Keys\n",
    "        self.base_agg_keys = ['card1', 'addr1']\n",
    "        \n",
    "        # 2. Prepare Aggregation Maps\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Map (for velocity/cardinality)\n",
    "            self.agg_features[f'{col}_Count_Map'] = X_copy[col].value_counts()\n",
    "            \n",
    "            # Amount Mean Map\n",
    "            self.agg_features[f'{col}_Amt_Mean_Map'] = X_copy.groupby(col)[self.amt_col].mean()\n",
    "            \n",
    "            # Amount Std Map\n",
    "            self.agg_features[f'{col}_Amt_Std_Map'] = X_copy.groupby(col)[self.amt_col].std().fillna(1.0)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # --- Time-Based Features (Requires TransactionDT) ---\n",
    "        if self.time_col in X_copy.columns:\n",
    "            # Hour of Day (0-23)\n",
    "            X_copy['Transaction_Hour'] = (X_copy[self.time_col] // 3600) % 24\n",
    "            # Day of Week (0-6)\n",
    "            X_copy['Transaction_DayOfWeek'] = (X_copy[self.time_col] // (3600 * 24)) % 7\n",
    "            X_copy['Transaction_Day'] = X_copy[self.time_col] // (3600 * 24)\n",
    "\n",
    "        # --- Aggregation and Ratio Features ---\n",
    "        for col in self.base_agg_keys:\n",
    "            # Frequency Feature\n",
    "            count_map = self.agg_features[f'{col}_Count_Map']\n",
    "            X_copy[f'{col}_Count'] = X_copy[col].map(count_map).fillna(0)\n",
    "            \n",
    "            # Amount Mean Feature\n",
    "            mean_map = self.agg_features[f'{col}_Amt_Mean_Map']\n",
    "            X_copy[f'{col}_Amt_Mean'] = X_copy[col].map(mean_map).fillna(X_copy[self.amt_col].mean())\n",
    "            \n",
    "            # Amount Std Feature\n",
    "            std_map = self.agg_features[f'{col}_Amt_Std_Map']\n",
    "            X_copy[f'{col}_Amt_Std'] = X_copy[col].map(std_map).fillna(1.0)\n",
    "        \n",
    "        # Amount-to-Mean Ratio (highly predictive)\n",
    "        X_copy['Amt_Div_Mean_card1'] = X_copy[self.amt_col] / X_copy['card1_Amt_Mean']\n",
    "        \n",
    "        # Log Transform\n",
    "        X_copy['TransactionAmt_Log'] = np.log1p(X_copy[self.amt_col])\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"../data/train_cleaned.csv\")\n",
    "df.drop(columns=[\"TransactionID\"], inplace=True)\n",
    "\n",
    "# Sort by time — IMPORTANT\n",
    "df = df.sort_values(by=\"TransactionDT\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. FEATURE GROUPS\n",
    "# =========================================================\n",
    "# NOTE: The new engineered features will automatically be picked up \n",
    "# by the numeric_cols list after the CustomFeatureEngineer step.\n",
    "\n",
    "nominal_cols = ['ProductCD', 'card4', 'card6']\n",
    "nominal_high = ['P_emaildomain', 'R_emaildomain', \"id_30\", \"id_31\", \"DeviceInfo\", \"id_33\"]\n",
    "ordinal_cols = ['M4', 'M2', 'M3', 'M5', 'M6']\n",
    "\n",
    "# Create a temporary list of numeric columns based on original data\n",
    "numeric_cols_original = []\n",
    "for col in df.columns:\n",
    "    if col not in nominal_cols and \\\n",
    "        col not in nominal_high and \\\n",
    "        col not in ordinal_cols and \\\n",
    "        col != \"isFraud\" and \\\n",
    "        df[col].dtype != \"object\":\n",
    "        numeric_cols_original.append(col)\n",
    "# We will use this list to initialize the `numeric_cols` after FE\n",
    "\n",
    "# =========================================================\n",
    "# 3. TIME-BASED SPLIT\n",
    "# =========================================================\n",
    "split_index = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_index].copy()\n",
    "test_df = df.iloc[split_index:].copy()\n",
    "\n",
    "X_train = train_df.drop(columns=[\"isFraud\"])\n",
    "y_train = train_df[\"isFraud\"]\n",
    "X_test = test_df.drop(columns=[\"isFraud\"])\n",
    "y_test = test_df[\"isFraud\"]\n",
    "\n",
    "# =========================================================\n",
    "# 3a. APPLY CUSTOM FEATURE ENGINEER (NEW STEP)\n",
    "# =========================================================\n",
    "fe = CustomFeatureEngineer()\n",
    "X_train = fe.fit_transform(X_train, y_train)\n",
    "X_test = fe.transform(X_test)\n",
    "\n",
    "# Update numeric_cols to include the new engineered features\n",
    "engineered_features = [\n",
    "    'Transaction_Hour', 'Transaction_DayOfWeek', 'Transaction_Day',\n",
    "    'card1_Count', 'card1_Amt_Mean', 'card1_Amt_Std',\n",
    "    'addr1_Count', 'addr1_Amt_Mean', 'addr1_Amt_Std',\n",
    "    'Amt_Div_Mean_card1', 'TransactionAmt_Log'\n",
    "]\n",
    "numeric_cols = numeric_cols_original + engineered_features\n",
    "\n",
    "# Drop the original TransactionDT after extracting time features\n",
    "if 'TransactionDT' in X_train.columns:\n",
    "    X_train.drop(columns=['TransactionDT'], inplace=True)\n",
    "    X_test.drop(columns=['TransactionDT'], inplace=True)\n",
    "    numeric_cols.remove('TransactionDT')\n",
    "\n",
    "# =========================================================\n",
    "# 4. Frequency Encoder (FIXED VERSION)\n",
    "# =========================================================\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self): # Fix: use double underscore\n",
    "        self.freq_maps = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        self.freq_maps = {}\n",
    "        for col in X.columns:\n",
    "            # Handle string conversion for mapping consistency\n",
    "            self.freq_maps[col] = X[col].astype(str).value_counts(normalize=True)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X)\n",
    "        X2 = X.copy()\n",
    "        for col in X.columns:\n",
    "            # Map based on string values and fill NaNs (new categories in test) with 0\n",
    "            X2[col] = X2[col].astype(str).map(self.freq_maps[col]).fillna(0)\n",
    "        return X2\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. COLUMN TRANSFORMER\n",
    "# =========================================================\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "freq = FrequencyEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"nominal\", ohe, nominal_cols),\n",
    "        (\"ordinal\", oe, ordinal_cols),\n",
    "        # NOTE: nominal_high columns (emails, device info) are now Frequency Encoded\n",
    "        (\"freq\", freq, nominal_high), \n",
    "        # NOTE: numeric_cols now includes the engineered features\n",
    "        (\"num\", scaler, [col for col in numeric_cols if col in X_train.columns]), \n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. APPLY PREPROCESSOR\n",
    "# =========================================================\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Get output feature names\n",
    "def get_feature_names(ct):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name != 'remainder':\n",
    "            # Check if the transformer is a OneHotEncoder to use get_feature_names_out\n",
    "            if hasattr(trans, \"get_feature_names_out\"):\n",
    "                names.extend(trans.get_feature_names_out(cols))\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "    return names\n",
    "\n",
    "train_cols = get_feature_names(preprocessor)\n",
    "\n",
    "x_train_trans = pd.DataFrame(X_train_pre, columns=train_cols)\n",
    "x_test_trans = pd.DataFrame(X_test_pre, columns=train_cols)\n",
    "\n",
    "print(\"Preprocessing completed. Shape:\", x_train_trans.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. PCA (FIT ONLY ON TRAIN — NO LEAKAGE)\n",
    "# =========================================================\n",
    "pca = PCA(n_components=0.90, random_state=42)\n",
    "x_train_pca = pca.fit_transform(x_train_trans)\n",
    "x_test_pca = pca.transform(x_test_trans)\n",
    "\n",
    "print(\"PCA completed. Train PCA shape:\", x_train_pca.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. MHSysGen AUGMENTATION\n",
    "# =========================================================\n",
    "df_aug = pd.DataFrame(x_train_trans.copy())\n",
    "df_aug[\"isFraud\"] = y_train.values\n",
    "\n",
    "mh = MHSysGen(method=\"parallel\", ratio=15, minority_class=1)\n",
    "\n",
    "print(\"\\nStarting MH-SysGen augmentation...\")\n",
    "X_aug, y_aug = mh.fit_resample(df_aug, target=\"isFraud\")\n",
    "print(\"Augmentation complete. New shape:\", X_aug.shape)\n",
    "\n",
    "# PCA transform on augmented data (NO FIT AGAIN!)\n",
    "X_aug_pca = pca.transform(X_aug)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. XGBOOST MODEL (Your hyperparameters)\n",
    "# =========================================================\n",
    "from catboost import CatBoostClassifier\n",
    "model_cat = CatBoostClassifier(iterations=1500,\n",
    "                           depth=15,\n",
    "                           learning_rate=0.1,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='F1',\n",
    "                           scale_pos_weight=15,\n",
    "                           random_seed=42,\n",
    "                           verbose=200, \n",
    "                           l2_leaf_reg= 0.007680412132464856, \n",
    "                           bagging_temperature= 0.2503415910965906,\n",
    "                           random_strength= 1.0004685436237792, \n",
    "                           border_count=153,\n",
    "    \n",
    ")\n",
    "print(\"Training catboost\")\n",
    "model_cat.fit(X_aug_pca, y_aug)\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators= 600, max_depth= 20, \n",
    "    min_child_weight= 9, learning_rate= 0.1, \n",
    "    subsample= 0.6322739922792978, \n",
    "    colsample_bytree= 0.789378175020648, \n",
    "    gamma= 0.5473581155773255, \n",
    "    reg_lambda= 0.0019094698638261935, \n",
    "    reg_alpha= 0.16454201700646068,\n",
    "    objective=\"binary:logistic\",\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=1,\n",
    "    # use_label_encoder=False is deprecated and removed in recent XGBoost versions.\n",
    "    eval_metric=\"logloss\" \n",
    ")\n",
    "print(\"training xgboost\")\n",
    "model_xgb.fit(X_aug_pca, y_aug)\n",
    "\n",
    "print(\"finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f6b2bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98    114044\n",
      "           1       0.48      0.45      0.46      4064\n",
      "\n",
      "    accuracy                           0.96    118108\n",
      "   macro avg       0.73      0.72      0.72    118108\n",
      "weighted avg       0.96      0.96      0.96    118108\n",
      "\n",
      "[[112060   1984]\n",
      " [  2233   1831]]\n",
      "roc-auc score 0.7165722721814511\n"
     ]
    }
   ],
   "source": [
    "p_xgb_train=model_xgb.predict_proba(x_train_pca)[:,1]\n",
    "p_cat_train=model_cat.predict_proba(x_train_pca)[:,1]\n",
    "stack_train=np.column_stack([p_xgb_train,p_cat_train])\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "meta_model=LogisticRegression(max_iter=1000)\n",
    "meta_model.fit(stack_train,y_train)\n",
    "\n",
    "p_xgb_test=model_xgb.predict_proba(x_test_pca)[:,1]\n",
    "p_cat_test=model_cat.predict_proba(x_test_pca)[:,1]\n",
    "stack_test=np.column_stack([p_xgb_test,p_cat_test])\n",
    "\n",
    "proba=meta_model.predict_proba(stack_test)[:,1]\n",
    "final_pred=(proba >=0.2).astype(int)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== FINAL RESULTS (FE + MH-SysGen + XGBoost) =====\")\n",
    "print(classification_report(y_test, final_pred))\n",
    "print(confusion_matrix(y_test,final_pred))\n",
    "from sklearn.metrics import roc_auc_score\n",
    "rc_score=roc_auc_score(y_test, final_pred)\n",
    "print(\"roc-auc score\",rc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129454",
   "metadata": {},
   "outputs": [],
   "source": [
    "##faster sequential code mhsysgen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class FastSequentialShadowMirror:\n",
    "    \"\"\"\n",
    "    FAST version of Sequential Shadow-Mirror oversampling.\n",
    "    Same ideology:\n",
    "      1. Shadow scaling\n",
    "      2. Mirror using nearest neighbor reflection\n",
    "      3. Vectorized & optimized (10-40x faster)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_col, minority_class=1, ratio=1.0):\n",
    "        self.target_col = target_col\n",
    "        self.minority_class = minority_class\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def fit_resample(self, df):\n",
    "        \"\"\"\n",
    "        df : DataFrame containing both features and target_col  \n",
    "        returns: X_resampled, y_resampled\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------\n",
    "        # PREPARE DATA\n",
    "        # ----------------------------\n",
    "        features = [c for c in df.columns if c != self.target_col]\n",
    "        minority_df = df[df[self.target_col] == self.minority_class]\n",
    "\n",
    "        if minority_df.empty:\n",
    "            raise ValueError(f\"Minority class {self.minority_class} not found.\")\n",
    "\n",
    "        total_synth = int(len(minority_df) * self.ratio)\n",
    "        if total_synth < 1:\n",
    "            raise ValueError(\"Ratio too small; produced 0 samples.\")\n",
    "\n",
    "        X_min = minority_df[features].values\n",
    "        n_features = X_min.shape[1]\n",
    "\n",
    "        # ----------------------------\n",
    "        # PHASE 1 — SHADOW (VECTORIZED)\n",
    "        # ----------------------------\n",
    "        idxs = np.random.randint(0, len(X_min), size=total_synth)\n",
    "        originals = X_min[idxs]\n",
    "\n",
    "        scales = np.random.uniform(0.6, 1.4, size=(total_synth, n_features))\n",
    "        shadows = originals * scales\n",
    "\n",
    "        # ----------------------------\n",
    "        # PHASE 2 — MIRROR (VECTORIZED KNN)\n",
    "        # ----------------------------\n",
    "        nbrs = NearestNeighbors(n_neighbors=2).fit(shadows)\n",
    "        _, idx_arrays = nbrs.kneighbors(shadows)\n",
    "\n",
    "        neighbors = shadows[idx_arrays[:, 1]]\n",
    "\n",
    "        reflect_scale = np.random.uniform(0.5, 1.3, size=(total_synth, 1))\n",
    "        mirrored = shadows + (neighbors - shadows) * reflect_scale\n",
    "\n",
    "        # ----------------------------\n",
    "        # BUILD SYNTHETIC DF\n",
    "        # ----------------------------\n",
    "        synthetic_df = pd.DataFrame(mirrored, columns=features)\n",
    "        synthetic_df[self.target_col] = self.minority_class\n",
    "\n",
    "        combined = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "\n",
    "        X_out = combined.drop(columns=[self.target_col])\n",
    "        y_out = combined[self.target_col]\n",
    "\n",
    "        return X_out, y_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
